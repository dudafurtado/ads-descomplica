1, Build, deploy, run e só?
Vamos entender as preocupações que devemos ter para entregar valor.

O fato de “na sua máquina funcionar” mas falhar em outras pode ter várias causas, e vou 
dividir os principais pontos para você verificar:

1. Dependências e Versões Diferentes

Linguagem/Runtime: A versão do Node, Python, Java, etc. pode ser diferente e quebrar 
partes do código.

Bibliotecas: Dependências instaladas globalmente na sua máquina, mas não no ambiente de 
produção ou outras máquinas.

Sistema Operacional: Algumas bibliotecas ou scripts podem funcionar diferente em Windows, 
macOS e Linux.

Como resolver:

Usar um arquivo de lock (package-lock.json, poetry.lock, requirements.txt).
Definir versão específica no .nvmrc (Node) ou .python-version.
Automatizar a instalação com scripts (npm ci, pip install -r requirements.txt).

2. Configurações e Variáveis de Ambiente

Sua máquina pode ter variáveis locais configuradas manualmente (.env) que não existem nas 
outras.
Configurações de banco, API keys, endpoints podem estar diferentes.

Como resolver:

Criar um arquivo .env.example para documentar todas as variáveis necessárias.
Usar ferramentas como Doppler, Vault ou dotenv para gerenciar segredos.

3. Dependências do Sistema

Pacotes como bibliotecas C, ferramentas CLI, drivers podem estar instalados na sua máquina 
e ausentes nas outras.

Exemplo: libpq-dev para PostgreSQL, build-essential para Node com addons nativos.

Como resolver:

Criar um arquivo Dockerfile ou script setup.sh com tudo que precisa ser instalado.
Documentar os requisitos no README.

4. Banco de Dados ou Serviços Externos

Conexões a bancos locais vs. bancos na nuvem.
Serviços externos (ex: APIs) que funcionam só na sua rede ou com permissões específicas.

Como resolver:

Usar ambientes de teste/staging que todos possam acessar.
Criar scripts para popular dados de teste (seeders).

5. Diferenças de Build/Deploy

No seu computador você roda npm start, mas em produção usa npm run build e o código quebra 
por causa de variáveis não setadas.
Arquivos ignorados no .gitignore podem estar presentes só localmente.

Como resolver:

Criar pipelines de CI/CD que testem e construam o projeto igual a produção.
Usar Docker para garantir o mesmo ambiente em qualquer lugar.

1. "O mínimo necessário para rodar no servidor"

Significado:
Essa frase remete à ideia de simplicidade e eficiência. O objetivo é colocar no servidor 
apenas o que é realmente necessário para a aplicação funcionar:

Dependências essenciais (bibliotecas, pacotes).

Configurações mínimas para execução.

Recursos de hardware e software adequados, mas sem excessos.

Por que é importante:
Isso reduz:

Riscos de falhas por componentes desnecessários.

Tempo de deploy.

Custos de infraestrutura.

Exemplo prático: em vez de instalar todo um ambiente de desenvolvimento no servidor, 
você empacota apenas o build final com as libs essenciais — como usar containers ou imagens 
minimalistas.

2. "Preocupação nas entregas além do código"

Significado:
Não basta o código “funcionar na sua máquina”. A entrega deve ser pensada como um processo 
completo, incluindo:

Documentação (como rodar, dependências).

Scripts de deploy automatizados.

Testes para garantir qualidade.

Observabilidade (logs, métricas, alertas).

Por que é importante:
Facilita a manutenção, a colaboração entre equipes e reduz riscos em produção.

Exemplo prático: usar pipelines de CI/CD para garantir que o mesmo código seja testado e 
entregue em diferentes ambientes com consistência.

3. "Interações humanas causam erro"

Significado:
Quanto mais passos manuais existirem, maior a chance de falhas. Automatizar processos é 
fundamental:

Deploys automáticos em vez de copiar arquivos manualmente.

Provisionamento de infraestrutura como código (Terraform, Ansible).

Scripts de setup para ambientes locais e servidores.

Por que é importante:
Automatização traz consistência e reprodutibilidade, reduzindo problemas do tipo “funciona aqui, 
mas não funciona lá”.

Exemplo prático: em vez de configurar o banco manualmente, usar scripts versionados que rodam 
iguais em qualquer ambiente.

2. Colaboração no código, como era?
Vamos entender o que motivou a inovação através de ferramnentas de colaboração.

1. Colaboração via e-mail

Antes de sistemas de controle de versão, os desenvolvedores trocavam arquivos-fonte por e-mail.
Cada pessoa editava localmente, salvava como arquivo .zip ou .tar e enviava para outra.
O risco de sobrescrever alterações era enorme, pois não havia controle de quem alterou o quê.
Muitas vezes alguém era responsável por “mesclar” manualmente os arquivos em uma versão final.

2. Software de bloqueio e controle centralizado

Com o tempo, surgiram sistemas como CVS (Concurrent Versions System) e Subversion (SVN).
A lógica era centralizada: havia um repositório único em um servidor, e os desenvolvedores faziam:
checkout para pegar o código
commit para enviar alterações
Para evitar conflitos, existia o lock: um arquivo era “travado” para edição, e só uma pessoa 
podia alterar de cada vez.
Isso reduzia conflitos, mas dificultava colaboração em paralelo.

3. Controle distribuído e ramificações (branches)

Com sistemas como Git (2005), o paradigma mudou para controle de versão distribuído:
Cada desenvolvedor tem uma cópia completa do repositório.
É possível criar branches para trabalhar em paralelo sem travar o trabalho dos outros.
Alterações são mescladas com ferramentas avançadas de merge e pull requests.
Hoje usamos plataformas colaborativas (GitHub, GitLab, Bitbucket) com:

Revisão de código

Automação de testes (CI/CD)
Gestão de issues e tarefas

3. SCM (Source Control Management)
Vamos entender o que é a gestão de código-fonte.

1. Ferramentas e evolução

Além do Git, temos várias ferramentas que marcaram épocas diferentes:
O RTC (Rational Team Concert), por exemplo:
Era um CVCS avançado: integrava controle de versão, gestão de mudanças, rastreabilidade 
e até gestão de projetos.
Permitindo stream e workspace para versionar e isolar mudanças, mas ainda centralizado.

2. Fluxo de merge e rollback

Em qualquer ferramenta madura, temos:

Branches / Streams:
Isolam funcionalidades, correções ou experimentos sem afetar a linha principal.

Merge:
Combina alterações. Em sistemas centralizados como RTC, o merge geralmente era mais manual 
e exigia disciplina.

No Git, temos:

Merge commit (mantém histórico da branch)
Rebase (histórico linear)

Rollback / Revert:
Possibilita voltar versões específicas sem afetar o histórico completo:

Git → git revert (gera commit de "desfazer")
RTC → Voltar ao snapshot ou baseline anterior

3. Versionamento e rastreabilidade

Todas essas ferramentas oferecem:
Histórico completo: quem alterou, quando, por quê.
Tags ou releases: para marcar versões estáveis.
Baselines / Snapshots: pontos seguros para rollback.

No RTC, você podia:

Criar um baseline antes de uma grande alteração.
Fazer rollback de um componente sem afetar o resto do código.
No Git, as tags e branches de release cumprem esse papel.

4. Maturidade do time para evitar perdas

Ter ferramentas poderosas não evita perda de código sozinho. É preciso maturidade e processo:
Padronização de branches
Ex: Git Flow, Trunk Based, GitHub Flow.
Evita caos com branches aleatórias.
Commits pequenos e frequentes
Facilita merges e rollbacks.
Code review e pull requests
RTC tinha deliveries revisados; Git usa PR/MR.
Integração Contínua (CI)
Garante que o código compilou/testou antes de integrar.
Automação de versões
Tags e releases automáticas → evitam "funciona na minha máquina".
Snapshots / Checkpoints
No RTC, era obrigatório; no Git, recomenda-se antes de mudanças grandes.

4. Gitlab: SCM + CI + CD
Apresentando o Gitlab como ferramenta de SCM e de CI e CD.

1. SCM – Source Code Management

O SCM é o gerenciamento de código-fonte, garantindo que o histórico, as versões e as 
colaborações no código sejam controlados.

No GitLab, isso significa:

Repositórios Git: armazenamento centralizado do código com versionamento.
Merge Requests (MRs): fluxo para revisão e aprovação de código antes de entrar na branch principal.
Branching e tagging: criação de versões estáveis, hotfixes e recursos isolados.
Proteção de branch: evita alterações diretas na main/master sem revisão.
Histórico completo: registro de quem mudou o quê, quando e por quê.
Benefício: garante rastreabilidade, colaboração e controle total sobre a evolução do código.

2. CI – Continuous Integration

A Integração Contínua (CI) é o processo de automatizar a construção e testes do código sempre 
que algo novo é enviado para o repositório. É o conjunto de ferramentas e metodologia de trabalho 
para gestão colaborativa de código-fonte até o “build” e testes automatizados

No GitLab CI:

Você escreve pipelines em .gitlab-ci.yml.
Pipelines podem ter jobs para:
Compilar o projeto
Rodar testes automatizados
Verificar qualidade de código (lint, segurança, cobertura)
Cada commit ou merge request pode disparar o pipeline para garantir que o código não 
quebre nada existente.

Benefício:

Detecta erros cedo
Garante qualidade e estabilidade
Evita “funciona na minha máquina, mas não no servidor”

3. CD – Continuous Delivery / Deployment

Continuous Delivery: o código está sempre pronto para ser implantado em produção com um clique, 
após testes e validações. ​É o conjunto de ferramentas e metodologia de trabalho para gestão da 
implantação de aplicações prontas para o ambiente produtivo

Continuous Deployment: leva isso além — cada alteração aprovada é implantada automaticamente, 
sem intervenção manual. ​É o conjunto de ferramentas e metodologia de trabalho para a entrega de 
aplicações em ambiente de testes para que as aplicações fiquem prontas para testes funcionais e 
prontas para entrega

No GitLab:

Mesma pipeline pode incluir etapas para:
Implantar em ambientes de staging
Implantar em produção (manual ou automático)
Fazer rollbacks se algo falhar

Benefício:

Entregas rápidas e frequentes
Menos risco em deploys grandes
Feedback rápido sobre problemas

4. Integração Total no GitLab

O diferencial é que GitLab une tudo:
SCM: versionamento e colaboração
CI/CD: pipelines automáticas
Segurança: análises de vulnerabilidade integradas
Monitoramento: feedback de performance pós-deploy

Isso cria um DevOps pipeline completo:
Código → Build → Test → Deploy → Monitorar → Melhorar

5. Automação com pipelines 
Benefícios de se automatizar uma pipeline para entrega de código na produção.

Uma pipeline é uma sequência automatizada de etapas que levam seu código desde o 
desenvolvimento até a entrega ou implantação final. É muito usada em DevOps e CI/CD 
para garantir que tudo aconteça de forma previsível, sem depender de processos manuais.

Componentes principais de uma pipeline

Source (Código-fonte)
A pipeline começa quando há uma mudança no repositório (ex: push no GitLab ou GitHub).

Build (Compilação)
O código é compilado ou transformado (por exemplo, TypeScript → JavaScript, Java → .jar).
Dependências são instaladas e artefatos são gerados.

Test (Testes automáticos)
Testes unitários, de integração ou end-to-end para garantir que o código não quebre 
funcionalidades existentes.

Deploy (Implantação)
O código aprovado é enviado para um ambiente de teste, homologação ou produção.

Monitoramento (Opcional)
Logs, métricas e alertas para verificar se o sistema está funcionando corretamente.

Benefícios

Automação → reduz erros humanos e tempo de entrega.
Padronização → garante que todas as alterações passem pelo mesmo processo.
Velocidade → facilita entregas rápidas e frequentes (CI/CD).
Rollback rápido → caso algo quebre, é fácil voltar para uma versão anterior.

Exemplo simples em GitLab CI/CD

stages:
  - build
  - test
  - deploy

build-job:
  stage: build
  script:
    - npm install
    - npm run build

test-job:
  stage: test
  script:
    - npm test

deploy-job:
  stage: deploy
  script:
    - npm run deploy
  when: manual

A automação da pipeline significa que todo o processo — desde quando o código é alterado 
até chegar em produção — acontece sem ações manuais desnecessárias. Ela usa ferramentas como 
GitLab CI/CD, GitHub Actions, Jenkins, Azure DevOps para rodar automaticamente sempre que um 
evento específico ocorre, como um commit ou merge request.

Vou explicar por partes:

1. Como a automação funciona

Gatilhos (Triggers)
A pipeline inicia automaticamente quando algo acontece no repositório:

Push para determinada branch
Merge Request aberto ou aprovado
Tag criada para nova versão

Agendamentos (ex: toda madrugada)

Arquivo de Configuração
No caso do GitLab, o arquivo .gitlab-ci.yml descreve todas as etapas (build, test, deploy) 
e as dependências entre elas.

Execução Automática
Servidores ou runners pegam o código, executam as instruções e enviam os resultados 
(logs, erros, status) de volta.

2. Benefícios da automação

CI (Integração Contínua) → cada alteração é testada e integrada automaticamente, evitando 
“surpresas” quando muitos códigos diferentes são juntados. ​É o conjunto de ferramentas e 
metodologia de trabalho para gestão colaborativa de código-fonte até o “build” e testes automatizados

CD (Entrega Contínua) → versões aprovadas podem ser implantadas automaticamente em ambientes 
de teste ou homologação.

Rollback Automático → se algo falhar, a automação pode voltar para a versão estável anterior 
sem intervenção manual.

Menos Erros Humanos → tarefas repetitivas (build, testes, deploy) acontecem sempre do mesmo jeito.

Velocidade e Frequência → times maduros podem fazer deploys diários ou até por commit.

3. Exemplo de Automação no GitLab
stages:
  - build
  - test
  - deploy

# Etapa de build automatizada
build-job:
  stage: build
  script:
    - npm ci
    - npm run build
  artifacts:
    paths:
      - dist/

# Etapa de testes automatizada
test-job:
  stage: test
  script:
    - npm run test
  needs: ["build-job"]

# Deploy automatizado
deploy-job:
  stage: deploy
  script:
    - npm run deploy
  only:
    - main  # Só roda na branch main


Aqui, o deploy só ocorre quando o código chega na branch principal, garantindo controle do fluxo.

4. Maturidade do Time

Para automação dar certo, o time precisa:
Padronizar commits e branches (ex: GitFlow ou trunk-based).
Escrever testes confiáveis para que a pipeline valide o código.
Criar processos de revisão (Code Review) antes do merge.
Monitorar logs e métricas para detectar falhas cedo.

6. Apresentando: Docker e Gitlab CI/CD
Vamos conhecer os nossos protagonistas.

​O Docker se caracteriza por ser uma ferramenta que permite a gestão de containers, podendo 
implantar em praticamente qualquer lugar, dada a compatibilidade com muitos sistemas operacionais
O Docker é uma plataforma que permite empacotar, distribuir e executar aplicações de forma 
padronizada, leve e isolada usando containers. Ele resolve problemas clássicos como: “funciona 
na minha máquina, mas não no servidor” ou “preciso instalar mil dependências para rodar”.

1. Conceito Principal

Com o Docker, a aplicação e todas as suas dependências (bibliotecas, configurações, binários, etc.) 
ficam dentro de um container.
Esse container roda sobre o Docker Engine, garantindo que a aplicação funcione do mesmo jeito em 
qualquer ambiente: desenvolvimento, teste, produção ou até mesmo em outro servidor.

2. Diferença de Máquina Virtual

Máquina Virtual (VM): Emula um sistema operacional completo. Pesa mais, consome mais recursos e 
demora para iniciar.
Container (Docker): Compartilha o mesmo kernel do host, mas isola os processos da aplicação. 
É mais leve, rápido e portátil.

3. Vantagens

Portabilidade: O mesmo container pode rodar no seu notebook, em um servidor físico ou na nuvem.
Escalabilidade: Fácil criar várias instâncias da mesma aplicação.
Rapidez: Containers sobem em segundos, diferente de VMs.
Consistência: Garante que a aplicação funciona igual em qualquer lugar.

4. Casos de Uso

Padronizar ambiente de desenvolvimento.
Criar ambientes de teste rápidos.
Facilitar CI/CD (integração e entrega contínua).
Rodar microsserviços isolados e independentes.

5. Automação com Docker

Dockerfile: Arquivo com os passos para construir a imagem do container (como instalar 
dependências e copiar o código).
Docker Compose: Orquestra múltiplos containers (ex.: backend, frontend, banco de dados) 
com um único comando.
Integrado facilmente a pipelines CI/CD (GitLab, GitHub Actions, Jenkins etc.).

Extra:

O modelo de Integração Contínua/Entrega Contínua (CI/CD) traz diversos benefícios 
significativos em comparação ao modelo tradicional de desenvolvimento como por exemplo 
os bugs e incompatibilidades que só eram descobertos no final do ciclo, quando era muito 
mais caro e complexo corrigi-los. Com CI/CD, os problemas são identificados diariamente 
através dos builds automatizados, permitindo correções imediatas quando o contexto ainda 
está fresco na mente dos desenvolvedores.

Em entregas menores e frequentes diminuem drasticamente o risco de falhas catastróficas. 
Se algo der errado, o impacto é limitado e o rollback é mais simples. No modelo antigo, 
uma falha na entrega trimestral poderia comprometer meses de trabalho.

Os usuários finais recebem funcionalidades mais rapidamente e podem fornecer feedback 
valioso que ainda pode ser incorporado no desenvolvimento. Isso evita o cenário clássico 
de entregar algo que já não atende às necessidades reais após meses de desenvolvimento.

A execução automatizada de testes a cada integração garante que a qualidade seja mantida 
constantemente, ao invés de ser verificada apenas no final. Isso resulta em software mais 
estável e confiável.

A integração diária força os desenvolvedores a comunicarem-se melhor e resolverem conflitos 
de código rapidamente, melhorando a sinergia da equipe.mCom entregas frequentes, é mais 
fácil estimar prazos e identificar gargalos no processo de desenvolvimento, proporcionando 
maior controle sobre o projeto.

O mercado muda rapidamente, e o modelo CI/CD permite adaptar-se a novas demandas sem perder 
meses de trabalho já investido em uma direção que pode ter se tornado obsoleta. Essencialmente, 
este modelo troca a ilusão de controle do modelo tradicional por controle real através de 
feedback constante e ajustes incrementais.

Resumo:

A eficiência operacional na entrega de software passou por uma grande evolução ao longo dos anos, 
impulsionada pela necessidade de maior agilidade, colaboração e automação. Antigamente, o processo 
de desenvolvimento e entrega de software era muitas vezes manual e propenso a erros. Os 
desenvolvedores trabalhavam isoladamente em seus próprios computadores, e a integração do código 
era feita de forma esporádica, geralmente no final do ciclo de desenvolvimento. Isso resultava em 
conflitos de código, bugs difíceis de rastrear e atrasos na entrega.

Com o tempo, surgiram ferramentas e práticas para otimizar esse processo. O controle de versão, 
por exemplo, tornou-se essencial para gerenciar as diferentes versões do código-fonte e permitir 
que vários desenvolvedores trabalhem simultaneamente no mesmo projeto. Ferramentas como o Git e 
plataformas como o GitLab surgiram como soluções populares para controle de versão e colaboração 
em projetos de software.

O GitLab, em particular, oferece uma plataforma completa para gerenciamento do ciclo de vida do 
desenvolvimento de software, indo além do controle de versão. Ele incorpora o conceito de CI/CD 
(Continuous Integration/Continuous Delivery/Continuous Deployment), que visa automatizar o processo 
de integração, teste e entrega de software.

A integração contínua (CI) é uma prática que incentiva os desenvolvedores a integrarem seu código 
com frequência, idealmente várias vezes ao dia. Cada integração é verificada por um build automatizado, 
que inclui testes para detectar erros o mais cedo possível. Isso garante que o código esteja sempre 
em um estado consistente e pronto para ser implantado.

A entrega contínua (CD) leva a automação um passo adiante, automatizando o processo de entrega do 
software em ambientes de teste e produção. Com o CD, os desenvolvedores podem implantar novas versões 
do software com mais frequência e com menos riscos, pois o processo de implantação é testado e 
automatizado.

A implantação contínua (também CD) é uma extensão da entrega contínua, em que cada alteração no 
código que passa pelos testes automatizados é automaticamente implantada em produção, sem intervenção 
manual. Isso permite que as empresas entreguem software com mais rapidez e frequência, respondendo às 
mudanças nas necessidades dos clientes com mais agilidade.

As pipelines de CI/CD são um elemento fundamental para a automação do processo de desenvolvimento e 
entrega de software. Uma pipeline é uma sequência de etapas automatizadas que são executadas em um 
determinado ordem. No contexto de CI/CD, as pipelines são usadas para automatizar tarefas como 
compilação do código, execução de testes, análise de código estático e implantação em diferentes 
ambientes.

O Docker é outra ferramenta que revolucionou a forma como o software é entregue. O Docker permite 
que os desenvolvedores empacotem suas aplicações e suas dependências em containers, que são unidades 
de software leves e portáteis. Os containers podem ser executados em qualquer máquina que tenha o 
Docker instalado, independentemente do sistema operacional host.

A combinação do GitLab CI/CD com o Docker permite criar fluxos de trabalho poderosos e eficientes 
para entrega de software. Os desenvolvedores podem usar o GitLab para criar pipelines que automatizam 
o processo de build, teste e implantação de containers Docker. Isso garante que o software seja 
entregue de forma consistente e confiável em diferentes ambientes.

Em resumo, a evolução da eficiência operacional na entrega de software passou por grandes avanços, 
impulsionados pela necessidade de maior agilidade, colaboração e automação. Ferramentas como Git, 
GitLab, Docker e práticas como CI/CD tornaram-se essenciais para o desenvolvimento e entrega de 
software modernos, permitindo que as empresas respondam às demandas do mercado com mais rapidez e 
eficiência.